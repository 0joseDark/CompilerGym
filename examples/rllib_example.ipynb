{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rllib-example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BoKmYo6oUci"
      },
      "source": [
        "License\n",
        "\n",
        "```\n",
        "Copyright (c) Facebook, Inc. and its affiliates.\n",
        "\n",
        "This source code is licensed under the MIT license found in the\n",
        "LICENSE file in the root directory of this source tree.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsrdt9HooN9K"
      },
      "source": [
        "# Using CompilerGym environments with RLlib\n",
        "\n",
        "In this notebook we will use [RLlib](https://docs.ray.io/en/master/rllib.html) to train an agent for CompilerGym's [LLVM environment](https://facebookresearch.github.io/CompilerGym/llvm/index.html). RLlib is a popular library for scalable reinforcement learning, built on [Ray](https://docs.ray.io/en/master/index.html). It provides distributed implementations of several standard reinforcement learning algorithms.\n",
        "\n",
        "Our goal is not to produce the best agent, but to demonstrate how to integrate CompilerGym with RLlib. It will take about 20 minutes to work through. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8CSxbx5ovuF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "We'll begin by installing the `compiler_gym` and `ray` packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT3QDtxbf3Cr",
        "outputId": "399fced8-ec0c-4745-cf29-74dac40429f4"
      },
      "source": [
        "!pip install compiler_gym 'ray[default,rllib]' &>/dev/null || echo \"Install failed!\"\n",
        "\n",
        "# Print the versions of the libraries that we are using:\n",
        "import compiler_gym\n",
        "import ray\n",
        "\n",
        "print(\"compiler_gym version:\", compiler_gym.__version__)\n",
        "print(\"ray version:\", ray.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compiler_gym version: 0.1.9\n",
            "ray version: 1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y2bt7GttpQ3"
      },
      "source": [
        "## Defining an Environment\n",
        "\n",
        "Next we will define the environment to use for our experiments. For the purposes of a simple demo we will apply two simplifying constraints to CompilerGym's LLVM environment:\n",
        "\n",
        "1. We will use only a small subset of the command line flag action space.\n",
        "2. We will clip the length of episodes to a maximum number of steps.\n",
        "\n",
        "To make things simple we will define a `make_env()` helper function to create our environment, and use the [compiler_gym.wrappers](https://facebookresearch.github.io/CompilerGym/compiler_gym/wrappers.html) API to implement these constraints. There is quite a lot going on in this cell, be sure to read through the comments for an explanation of what is going on!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNeq2sArf5pa"
      },
      "source": [
        "from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit\n",
        "from ray import tune\n",
        "\n",
        "def make_env() -> compiler_gym.envs.CompilerEnv:\n",
        "    \"\"\"Make the reinforcement learning environment for this experiment.\"\"\"\n",
        "    # We will use LLVM as our base environment. Here we specify the observation\n",
        "    # space from this paper: https://arxiv.org/pdf/2003.00671.pdf and the total\n",
        "    # IR instruction count as our reward space, normalized against the \n",
        "    # performance of LLVM's -Oz policy.\n",
        "    env = compiler_gym.make(\n",
        "        \"llvm-v0\",\n",
        "        observation_space=\"Autophase\",\n",
        "        reward_space=\"IrInstructionCountOz\",\n",
        "    )\n",
        "    # Here we constrain the action space of the environment to use only a \n",
        "    # handful of command line flags from the full set. We do this to speed up\n",
        "    # learning by pruning the action space by hand. This also limits the \n",
        "    # potential improvements that the agent can achieve compared to using the \n",
        "    # full action space.\n",
        "    env = ConstrainedCommandline(env, flags=[\n",
        "        \"-break-crit-edges\",\n",
        "        \"-early-cse-memssa\",\n",
        "        \"-gvn-hoist\",\n",
        "        \"-gvn\",\n",
        "        \"-instcombine\",\n",
        "        \"-instsimplify\",\n",
        "        \"-jump-threading\",\n",
        "        \"-loop-reduce\",\n",
        "        \"-loop-rotate\",\n",
        "        \"-loop-versioning\",\n",
        "        \"-mem2reg\",\n",
        "        \"-newgvn\",\n",
        "        \"-reg2mem\",\n",
        "        \"-simplifycfg\",\n",
        "        \"-sroa\",\n",
        "    ])\n",
        "    # Finally, we impose a time limit on the environment so that every episode\n",
        "    # for 5 steps or fewer. This is because the environment's task is continuous\n",
        "    # and no action is guaranteed to result in a terminal state. Adding a time\n",
        "    # limit means we don't have to worry about learning when an agent should \n",
        "    # stop, though again this limits the potential improvements that the agent\n",
        "    # can achieve compared to using an unbounded maximum episode length.\n",
        "    env = TimeLimit(env, max_episode_steps=5)\n",
        "    return env"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg84RXsDt5ey",
        "outputId": "8d1fa820-348e-400e-de6c-0a6d9a14d9bb"
      },
      "source": [
        "# Let's create an environment and print a few attributes just to check that we \n",
        "# have everything set up the way that we would like.\n",
        "with make_env() as env:\n",
        "    print(\"Action space:\", env.action_space)\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Reward space:\", env.reward_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action space: Commandline([-break-crit-edges -early-cse-memssa -gvn-hoist -gvn -instcombine -instsimplify -jump-threading -loop-reduce -loop-rotate -loop-versioning -mem2reg -newgvn -reg2mem -simplifycfg -sroa])\n",
            "Observation space: Box(0, 9223372036854775807, (56,), int64)\n",
            "Reward space: IrInstructionCountOz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39k80F8itTT_"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Now that we have an environment, we will need a set of programs to train on. In CompilerGym, these programs are called *benchmarks*. CompilerGym ships with [several sets of benchmarks](https://facebookresearch.github.io/CompilerGym/llvm/index.html#datasets). Here we will take a handful of benchmarks from the `npb-v0` dataset for training. We will then further divide this set into training and validation sets. We will use `chstone-v0` as a holdout test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV6-adNIhoGS",
        "outputId": "23afc569-58d1-443a-a4a7-e72b703152cc"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "with make_env() as env:\n",
        "  # The two datasets we will be using:\n",
        "  npb = env.datasets[\"npb-v0\"]\n",
        "  chstone = env.datasets[\"chstone-v0\"]\n",
        "\n",
        "  # Each dataset has a `benchmarks()` method that returns an iterator over the\n",
        "  # benchmarks within the dataset. Here we will use iterator sliceing to grab a \n",
        "  # handful of benchmarks for training and validation.\n",
        "  train_benchmarks = list(islice(npb.benchmarks(), 55))\n",
        "  train_benchmarks, val_benchmarks = train_benchmarks[:50], train_benchmarks[50:]\n",
        "  # We will use the entire chstone-v0 dataset for testing.\n",
        "  test_benchmarks = list(chstone.benchmarks())\n",
        "\n",
        "print(\"Number of benchmarks for training:\", len(train_benchmarks))\n",
        "print(\"Number of benchmarks for validation:\", len(val_benchmarks))\n",
        "print(\"Number of benchmarks for testing:\", len(test_benchmarks))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of benchmarks for training: 50\n",
            "Number of benchmarks for validation: 5\n",
            "Number of benchmarks for testing: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW0sfMhjv8Kg"
      },
      "source": [
        "## Registering the environment with RLlib\n",
        "\n",
        "Now that we have our environment and training benchmarks, we can register the environment for use with RLlib. To do this we will define a second `make_training_env()` helper that uses the [CycleOverBenchmarks](https://facebookresearch.github.io/CompilerGym/compiler_gym/wrappers.html#compiler_gym.wrappers.CycleOverBenchmarks) wrapper to ensure that the environment uses all of the training benchmarks. We then call `tune.register_env()`, assining the environment a name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UgFKvTkv64L"
      },
      "source": [
        "from compiler_gym.wrappers import CycleOverBenchmarks\n",
        "\n",
        "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
        "  \"\"\"Make a reinforcement learning environment that cycles over the\n",
        "  set of training benchmarks in use.\n",
        "  \"\"\"\n",
        "  del args  # Unused env_config argument passed by ray\n",
        "  return CycleOverBenchmarks(make_env(), train_benchmarks)\n",
        "\n",
        "tune.register_env(\"compiler_gym\", make_training_env)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CVCAIrpyKa4",
        "outputId": "76537a51-91e7-46c5-ab7c-e71122be04c6"
      },
      "source": [
        "# Lets cycle through a few calls to reset() to demonstrate that this environment\n",
        "# selects a new benchmark for each episode.\n",
        "with make_training_env() as env:\n",
        "  env.reset()\n",
        "  print(env.benchmark)\n",
        "  env.reset()\n",
        "  print(env.benchmark)\n",
        "  env.reset()\n",
        "  print(env.benchmark)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "benchmark://npb-v0/1\n",
            "benchmark://npb-v0/2\n",
            "benchmark://npb-v0/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpR63LcOuRRz"
      },
      "source": [
        "## Run the training loop\n",
        "\n",
        "Now that we have the environment set up, let's run a training loop. Here will use RLlib's [Proximal Policy Optimization](https://docs.ray.io/en/master/rllib-algorithms.html#ppo) implementation, and run a very short training loop just for demonstative purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYGUxq6GhXZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ce9e698-6fd2-442d-976d-40c4c0a0902f"
      },
      "source": [
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# (Re)Start the ray runtime.\n",
        "if ray.is_initialized():\n",
        "  ray.shutdown()\n",
        "ray.init(include_dashboard=False, ignore_reinit_error=True)\n",
        "\n",
        "tune.register_env(\"compiler_gym\", make_training_env)\n",
        "\n",
        "analysis = tune.run(\n",
        "    PPOTrainer,\n",
        "    checkpoint_at_end=True,\n",
        "    stop={\n",
        "        \"episodes_total\": 500,\n",
        "    },\n",
        "    config={\n",
        "        \"seed\": 0xCC,\n",
        "        \"num_workers\": 1,\n",
        "        # Specify the environment to use, where \"compiler_gym\" is the name we \n",
        "        # passed to tune.register_env().\n",
        "        \"env\": \"compiler_gym\",\n",
        "        # Reduce the size of the batch/trajectory lengths to match our short \n",
        "        # training run.\n",
        "        \"rollout_fragment_length\": 5,\n",
        "        \"train_batch_size\": 5,\n",
        "        \"sgd_minibatch_size\": 5,\n",
        "    }\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_compile is deprecated, use jit_compile instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m experimental_compile is deprecated, use jit_compile instead\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m 2021-06-04 16:03:42,146\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m 2021-06-04 16:03:42,146\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=9273)\u001b[0m experimental_compile is deprecated, use jit_compile instead\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m 2021-06-04 16:03:48,888\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m 2021-06-04 16:03:49,136\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 5\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-03-49\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.4069767441860465\n",
            "  episode_reward_mean: 0.4069767441860465\n",
            "  episode_reward_min: 0.4069767441860465\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 1\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.20000000298023224\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.7013802528381348\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.006312782876193523\n",
            "          model: {}\n",
            "          policy_loss: -0.18230918049812317\n",
            "          total_loss: -0.15246529877185822\n",
            "          vf_explained_var: 0.16006684303283691\n",
            "          vf_loss: 0.028581315651535988\n",
            "    num_agent_steps_sampled: 5\n",
            "    num_steps_sampled: 5\n",
            "    num_steps_trained: 5\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 60.2\n",
            "    ram_util_percent: 15.6\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11428197224934895\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 12.464682261149088\n",
            "    mean_inference_ms: 6.037394205729166\n",
            "    mean_raw_obs_processing_ms: 18.34575335184733\n",
            "  time_since_restore: 0.5937602519989014\n",
            "  time_this_iter_s: 0.5937602519989014\n",
            "  time_total_s: 0.5937602519989014\n",
            "  timers:\n",
            "    learn_throughput: 17.016\n",
            "    learn_time_ms: 293.844\n",
            "    sample_throughput: 20.905\n",
            "    sample_time_ms: 239.183\n",
            "    update_time_ms: 3.995\n",
            "  timestamp: 1622822629\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 5\n",
            "  training_iteration: 1\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         0.59376</td><td style=\"text-align: right;\">   5</td><td style=\"text-align: right;\">0.406977</td><td style=\"text-align: right;\">            0.406977</td><td style=\"text-align: right;\">            0.406977</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 75\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-03-54\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 9.431818181818182\n",
            "  episode_reward_mean: 1.2791555852475684\n",
            "  episode_reward_min: -0.08085106382978724\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 15\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.708593726158142\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.366990566253662\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.015009311959147453\n",
            "          model: {}\n",
            "          policy_loss: -0.2545779347419739\n",
            "          total_loss: 0.18985310196876526\n",
            "          vf_explained_var: 0.0\n",
            "          vf_loss: 0.4187861979007721\n",
            "    num_agent_steps_sampled: 75\n",
            "    num_steps_sampled: 75\n",
            "    num_steps_trained: 75\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.4\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10154629129761522\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.525614429953598\n",
            "    mean_inference_ms: 2.4697286285748574\n",
            "    mean_raw_obs_processing_ms: 25.977244443966377\n",
            "  time_since_restore: 5.461083889007568\n",
            "  time_this_iter_s: 0.26230716705322266\n",
            "  time_total_s: 5.461083889007568\n",
            "  timers:\n",
            "    learn_throughput: 44.882\n",
            "    learn_time_ms: 111.403\n",
            "    sample_throughput: 27.45\n",
            "    sample_time_ms: 182.151\n",
            "    update_time_ms: 2.597\n",
            "  timestamp: 1622822634\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 75\n",
            "  training_iteration: 15\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5.46108</td><td style=\"text-align: right;\">  75</td><td style=\"text-align: right;\"> 1.27916</td><td style=\"text-align: right;\">             9.43182</td><td style=\"text-align: right;\">          -0.0808511</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 145\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-03-59\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 9.431818181818182\n",
            "  episode_reward_mean: -0.8422814711871025\n",
            "  episode_reward_min: -52.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 29\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.324877738952637\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.3182315826416016\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.010489774867892265\n",
            "          model: {}\n",
            "          policy_loss: -0.20667710900306702\n",
            "          total_loss: 0.3873479962348938\n",
            "          vf_explained_var: 0.49481141567230225\n",
            "          vf_loss: 0.5486580729484558\n",
            "    num_agent_steps_sampled: 145\n",
            "    num_steps_sampled: 145\n",
            "    num_steps_trained: 145\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.9\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09784851680709857\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 3.56726833943414\n",
            "    mean_inference_ms: 2.0198874288165136\n",
            "    mean_raw_obs_processing_ms: 27.789316794451835\n",
            "  time_since_restore: 10.397522211074829\n",
            "  time_this_iter_s: 0.5686323642730713\n",
            "  time_total_s: 10.397522211074829\n",
            "  timers:\n",
            "    learn_throughput: 45.342\n",
            "    learn_time_ms: 110.272\n",
            "    sample_throughput: 28.958\n",
            "    sample_time_ms: 172.666\n",
            "    update_time_ms: 2.816\n",
            "  timestamp: 1622822639\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 145\n",
            "  training_iteration: 29\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         10.3975</td><td style=\"text-align: right;\"> 145</td><td style=\"text-align: right;\">-0.842281</td><td style=\"text-align: right;\">             9.43182</td><td style=\"text-align: right;\">                 -52</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 215\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-05\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 3.394686556111372\n",
            "  episode_reward_min: -52.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 43\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.2436585426330566\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.3825843334198\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.022935297340154648\n",
            "          model: {}\n",
            "          policy_loss: -0.2687135636806488\n",
            "          total_loss: 0.24539396166801453\n",
            "          vf_explained_var: 0.6805019378662109\n",
            "          vf_loss: 0.4397132992744446\n",
            "    num_agent_steps_sampled: 215\n",
            "    num_steps_sampled: 215\n",
            "    num_steps_trained: 215\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09602175375025326\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 3.1979974575236976\n",
            "    mean_inference_ms: 1.8297934814174246\n",
            "    mean_raw_obs_processing_ms: 28.448365874837314\n",
            "  time_since_restore: 15.140862703323364\n",
            "  time_this_iter_s: 0.28540873527526855\n",
            "  time_total_s: 15.140862703323364\n",
            "  timers:\n",
            "    learn_throughput: 45.726\n",
            "    learn_time_ms: 109.347\n",
            "    sample_throughput: 30.533\n",
            "    sample_time_ms: 163.758\n",
            "    update_time_ms: 3.009\n",
            "  timestamp: 1622822645\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 215\n",
            "  training_iteration: 43\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         15.1409</td><td style=\"text-align: right;\"> 215</td><td style=\"text-align: right;\"> 3.39469</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                 -52</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 305\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-10\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 2.886634196406002\n",
            "  episode_reward_min: -52.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 61\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.649115800857544\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.1408872604370117\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.007102621253579855\n",
            "          model: {}\n",
            "          policy_loss: -0.21200492978096008\n",
            "          total_loss: 0.2864999771118164\n",
            "          vf_explained_var: 0.7792697548866272\n",
            "          vf_loss: 0.4725866913795471\n",
            "    num_agent_steps_sampled: 305\n",
            "    num_steps_sampled: 305\n",
            "    num_steps_trained: 305\n",
            "  iterations_since_restore: 61\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 74.8\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09413389455764233\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.926888103699783\n",
            "    mean_inference_ms: 1.6985647933180148\n",
            "    mean_raw_obs_processing_ms: 28.16769681802115\n",
            "  time_since_restore: 19.79714035987854\n",
            "  time_this_iter_s: 0.2480299472808838\n",
            "  time_total_s: 19.79714035987854\n",
            "  timers:\n",
            "    learn_throughput: 40.409\n",
            "    learn_time_ms: 123.734\n",
            "    sample_throughput: 249.459\n",
            "    sample_time_ms: 20.043\n",
            "    update_time_ms: 2.813\n",
            "  timestamp: 1622822650\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 305\n",
            "  training_iteration: 61\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         19.7971</td><td style=\"text-align: right;\"> 305</td><td style=\"text-align: right;\"> 2.88663</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                 -52</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 415\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-15\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 4.267204983307606\n",
            "  episode_reward_min: -52.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 83\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.4736738204956055\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.9793866872787476\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003541119396686554\n",
            "          model: {}\n",
            "          policy_loss: -0.11942613124847412\n",
            "          total_loss: 0.10622720420360565\n",
            "          vf_explained_var: 0.6420673131942749\n",
            "          vf_loss: 0.20627036690711975\n",
            "    num_agent_steps_sampled: 415\n",
            "    num_steps_sampled: 415\n",
            "    num_steps_trained: 415\n",
            "  iterations_since_restore: 83\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09233872712808458\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.7213101196878586\n",
            "    mean_inference_ms: 1.6034827347572693\n",
            "    mean_raw_obs_processing_ms: 26.15852066577332\n",
            "  time_since_restore: 24.515944480895996\n",
            "  time_this_iter_s: 0.2307298183441162\n",
            "  time_total_s: 24.515944480895996\n",
            "  timers:\n",
            "    learn_throughput: 40.879\n",
            "    learn_time_ms: 122.312\n",
            "    sample_throughput: 230.24\n",
            "    sample_time_ms: 21.717\n",
            "    update_time_ms: 3.07\n",
            "  timestamp: 1622822655\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 415\n",
            "  training_iteration: 83\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         24.5159</td><td style=\"text-align: right;\"> 415</td><td style=\"text-align: right;\">  4.2672</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                 -52</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 525\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-20\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 4.664450347937849\n",
            "  episode_reward_min: -52.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 105\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.1546030044555664\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.6399190425872803\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.021851975470781326\n",
            "          model: {}\n",
            "          policy_loss: -0.1908329874277115\n",
            "          total_loss: -0.16160084307193756\n",
            "          vf_explained_var: 0.9634737968444824\n",
            "          vf_loss: 0.004001783672720194\n",
            "    num_agent_steps_sampled: 525\n",
            "    num_steps_sampled: 525\n",
            "    num_steps_trained: 525\n",
            "  iterations_since_restore: 105\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.0\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09046839729430642\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.3751495457122984\n",
            "    mean_inference_ms: 1.4378708856363303\n",
            "    mean_raw_obs_processing_ms: 24.083908567620504\n",
            "  time_since_restore: 29.136346578598022\n",
            "  time_this_iter_s: 0.23457860946655273\n",
            "  time_total_s: 29.136346578598022\n",
            "  timers:\n",
            "    learn_throughput: 43.431\n",
            "    learn_time_ms: 115.125\n",
            "    sample_throughput: 233.682\n",
            "    sample_time_ms: 21.397\n",
            "    update_time_ms: 2.841\n",
            "  timestamp: 1622822660\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 525\n",
            "  training_iteration: 105\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         29.1363</td><td style=\"text-align: right;\"> 525</td><td style=\"text-align: right;\"> 4.66445</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                 -52</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 635\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-25\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 6.085601770717566\n",
            "  episode_reward_min: -2.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 127\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.4659345149993896\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.619084119796753\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.006216425448656082\n",
            "          model: {}\n",
            "          policy_loss: -0.047797299921512604\n",
            "          total_loss: -0.005831587128341198\n",
            "          vf_explained_var: 0.9922365546226501\n",
            "          vf_loss: 0.026636406779289246\n",
            "    num_agent_steps_sampled: 635\n",
            "    num_steps_sampled: 635\n",
            "    num_steps_trained: 635\n",
            "  iterations_since_restore: 127\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0882221998380064\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.1989562081891014\n",
            "    mean_inference_ms: 1.349992762793174\n",
            "    mean_raw_obs_processing_ms: 20.626524673147536\n",
            "  time_since_restore: 33.80850076675415\n",
            "  time_this_iter_s: 0.22116780281066895\n",
            "  time_total_s: 33.80850076675415\n",
            "  timers:\n",
            "    learn_throughput: 44.671\n",
            "    learn_time_ms: 111.93\n",
            "    sample_throughput: 243.194\n",
            "    sample_time_ms: 20.56\n",
            "    update_time_ms: 2.947\n",
            "  timestamp: 1622822665\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 635\n",
            "  training_iteration: 127\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         33.8085</td><td style=\"text-align: right;\"> 635</td><td style=\"text-align: right;\">  6.0856</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 740\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-30\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 153.0\n",
            "  episode_reward_mean: 5.397098575733069\n",
            "  episode_reward_min: -2.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 148\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.21944166719913483\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.6173404455184937\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0002538803964853287\n",
            "          model: {}\n",
            "          policy_loss: -0.008465206250548363\n",
            "          total_loss: 2.2837562561035156\n",
            "          vf_explained_var: -0.3163095712661743\n",
            "          vf_loss: 2.292165756225586\n",
            "    num_agent_steps_sampled: 740\n",
            "    num_steps_sampled: 740\n",
            "    num_steps_trained: 740\n",
            "  iterations_since_restore: 148\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08705836630442165\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.1319288642979344\n",
            "    mean_inference_ms: 1.3211406071327176\n",
            "    mean_raw_obs_processing_ms: 16.729900128342347\n",
            "  time_since_restore: 38.396830797195435\n",
            "  time_this_iter_s: 0.20396137237548828\n",
            "  time_total_s: 38.396830797195435\n",
            "  timers:\n",
            "    learn_throughput: 43.437\n",
            "    learn_time_ms: 115.109\n",
            "    sample_throughput: 230.13\n",
            "    sample_time_ms: 21.727\n",
            "    update_time_ms: 2.885\n",
            "  timestamp: 1622822670\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 740\n",
            "  training_iteration: 148\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         38.3968</td><td style=\"text-align: right;\"> 740</td><td style=\"text-align: right;\">  5.3971</td><td style=\"text-align: right;\">                 153</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 845\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-36\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 5.4284459237557\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 169\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.005492236465215683\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.13354754447937\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.046996910125017166\n",
            "          model: {}\n",
            "          policy_loss: -0.14283575117588043\n",
            "          total_loss: 3.297272205352783\n",
            "          vf_explained_var: 0.9391043782234192\n",
            "          vf_loss: 3.439849853515625\n",
            "    num_agent_steps_sampled: 845\n",
            "    num_steps_sampled: 845\n",
            "    num_steps_trained: 845\n",
            "  iterations_since_restore: 169\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 76.8\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08665486115790273\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.098115951025535\n",
            "    mean_inference_ms: 1.3052562290358112\n",
            "    mean_raw_obs_processing_ms: 13.493105317733061\n",
            "  time_since_restore: 43.100391149520874\n",
            "  time_this_iter_s: 0.24580121040344238\n",
            "  time_total_s: 43.100391149520874\n",
            "  timers:\n",
            "    learn_throughput: 41.265\n",
            "    learn_time_ms: 121.167\n",
            "    sample_throughput: 218.713\n",
            "    sample_time_ms: 22.861\n",
            "    update_time_ms: 3.045\n",
            "  timestamp: 1622822676\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 845\n",
            "  training_iteration: 169\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         43.1004</td><td style=\"text-align: right;\"> 845</td><td style=\"text-align: right;\"> 5.42845</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 950\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-41\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 5.438693288465101\n",
            "  episode_reward_min: -0.007751937984496124\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 190\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.4050166606903076\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.4306457042694092\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.02172720432281494\n",
            "          model: {}\n",
            "          policy_loss: -0.1413513869047165\n",
            "          total_loss: 0.332834392786026\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 0.42193156480789185\n",
            "    num_agent_steps_sampled: 950\n",
            "    num_steps_sampled: 950\n",
            "    num_steps_trained: 950\n",
            "  iterations_since_restore: 190\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08653842689709908\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.0857692532072685\n",
            "    mean_inference_ms: 1.2995526296137052\n",
            "    mean_raw_obs_processing_ms: 11.397481658325237\n",
            "  time_since_restore: 47.8090877532959\n",
            "  time_this_iter_s: 0.23538947105407715\n",
            "  time_total_s: 47.8090877532959\n",
            "  timers:\n",
            "    learn_throughput: 44.195\n",
            "    learn_time_ms: 113.136\n",
            "    sample_throughput: 236.301\n",
            "    sample_time_ms: 21.159\n",
            "    update_time_ms: 3.005\n",
            "  timestamp: 1622822681\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 950\n",
            "  training_iteration: 190\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         47.8091</td><td style=\"text-align: right;\"> 950</td><td style=\"text-align: right;\"> 5.43869</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">         -0.00775194</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1055\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-46\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 5.449550297250294\n",
            "  episode_reward_min: -0.007751937984496124\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 211\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.705643892288208\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.3432374000549316\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00826875027269125\n",
            "          model: {}\n",
            "          policy_loss: -0.17243722081184387\n",
            "          total_loss: 1.9828155040740967\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 2.132880449295044\n",
            "    num_agent_steps_sampled: 1055\n",
            "    num_steps_sampled: 1055\n",
            "    num_steps_trained: 1055\n",
            "  iterations_since_restore: 211\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 75.7\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0864145581625899\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.0742790106420577\n",
            "    mean_inference_ms: 1.2957947456930856\n",
            "    mean_raw_obs_processing_ms: 9.91609228360655\n",
            "  time_since_restore: 52.45294523239136\n",
            "  time_this_iter_s: 0.23391151428222656\n",
            "  time_total_s: 52.45294523239136\n",
            "  timers:\n",
            "    learn_throughput: 43.735\n",
            "    learn_time_ms: 114.324\n",
            "    sample_throughput: 232.923\n",
            "    sample_time_ms: 21.466\n",
            "    update_time_ms: 3.896\n",
            "  timestamp: 1622822686\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1055\n",
            "  training_iteration: 211\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         52.4529</td><td style=\"text-align: right;\">1055</td><td style=\"text-align: right;\"> 5.44955</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">         -0.00775194</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1160\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-51\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 6.0502116898276315\n",
            "  episode_reward_min: -0.007751937984496124\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 232\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.5682477951049805\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.201913356781006\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.023324552923440933\n",
            "          model: {}\n",
            "          policy_loss: -0.17493696510791779\n",
            "          total_loss: 0.24101801216602325\n",
            "          vf_explained_var: 0.953052818775177\n",
            "          vf_loss: 0.3560517430305481\n",
            "    num_agent_steps_sampled: 1160\n",
            "    num_steps_sampled: 1160\n",
            "    num_steps_trained: 1160\n",
            "  iterations_since_restore: 232\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08626257735215036\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.064777256172437\n",
            "    mean_inference_ms: 1.2938289702869215\n",
            "    mean_raw_obs_processing_ms: 8.807478559949093\n",
            "  time_since_restore: 57.181453227996826\n",
            "  time_this_iter_s: 0.22298026084899902\n",
            "  time_total_s: 57.181453227996826\n",
            "  timers:\n",
            "    learn_throughput: 42.351\n",
            "    learn_time_ms: 118.061\n",
            "    sample_throughput: 232.956\n",
            "    sample_time_ms: 21.463\n",
            "    update_time_ms: 2.753\n",
            "  timestamp: 1622822691\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1160\n",
            "  training_iteration: 232\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         57.1815</td><td style=\"text-align: right;\">1160</td><td style=\"text-align: right;\"> 6.05021</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">         -0.00775194</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1260\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-04-56\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 159.0\n",
            "  episode_reward_mean: 6.087637708895921\n",
            "  episode_reward_min: -0.007751937984496124\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 252\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.60945725440979\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.3165415525436401\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.010732493363320827\n",
            "          model: {}\n",
            "          policy_loss: -0.14077933132648468\n",
            "          total_loss: -0.06813585758209229\n",
            "          vf_explained_var: 0.812760055065155\n",
            "          vf_loss: 0.06610246747732162\n",
            "    num_agent_steps_sampled: 1260\n",
            "    num_steps_sampled: 1260\n",
            "    num_steps_trained: 1260\n",
            "  iterations_since_restore: 252\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08618385914009849\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.053906489591374\n",
            "    mean_inference_ms: 1.2909911612439078\n",
            "    mean_raw_obs_processing_ms: 7.979698880683576\n",
            "  time_since_restore: 61.746395111083984\n",
            "  time_this_iter_s: 0.225541353225708\n",
            "  time_total_s: 61.746395111083984\n",
            "  timers:\n",
            "    learn_throughput: 40.873\n",
            "    learn_time_ms: 122.331\n",
            "    sample_throughput: 244.211\n",
            "    sample_time_ms: 20.474\n",
            "    update_time_ms: 2.823\n",
            "  timestamp: 1622822696\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1260\n",
            "  training_iteration: 252\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         61.7464</td><td style=\"text-align: right;\">1260</td><td style=\"text-align: right;\"> 6.08764</td><td style=\"text-align: right;\">                 159</td><td style=\"text-align: right;\">         -0.00775194</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1360\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-01\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 150.0\n",
            "  episode_reward_mean: 5.790035518450664\n",
            "  episode_reward_min: -0.007751937984496124\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 272\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.36608725786209106\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.8189104795455933\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.009806491434574127\n",
            "          model: {}\n",
            "          policy_loss: -0.09162213653326035\n",
            "          total_loss: 47.403053283691406\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 47.491085052490234\n",
            "    num_agent_steps_sampled: 1360\n",
            "    num_steps_sampled: 1360\n",
            "    num_steps_trained: 1360\n",
            "  iterations_since_restore: 272\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.5\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08603560280899608\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.04386623599296\n",
            "    mean_inference_ms: 1.2882340692004157\n",
            "    mean_raw_obs_processing_ms: 7.309027550623734\n",
            "  time_since_restore: 66.41533923149109\n",
            "  time_this_iter_s: 0.24122977256774902\n",
            "  time_total_s: 66.41533923149109\n",
            "  timers:\n",
            "    learn_throughput: 42.995\n",
            "    learn_time_ms: 116.292\n",
            "    sample_throughput: 223.217\n",
            "    sample_time_ms: 22.4\n",
            "    update_time_ms: 3.007\n",
            "  timestamp: 1622822701\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1360\n",
            "  training_iteration: 272\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         66.4153</td><td style=\"text-align: right;\">1360</td><td style=\"text-align: right;\"> 5.79004</td><td style=\"text-align: right;\">                 150</td><td style=\"text-align: right;\">         -0.00775194</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1455\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-06\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 150.0\n",
            "  episode_reward_mean: 5.866413579548341\n",
            "  episode_reward_min: -0.1410459587955626\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 291\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.32985055446624756\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4789682924747467\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004818943329155445\n",
            "          model: {}\n",
            "          policy_loss: -0.03120868280529976\n",
            "          total_loss: 0.35436782240867615\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 0.3839869797229767\n",
            "    num_agent_steps_sampled: 1455\n",
            "    num_steps_sampled: 1455\n",
            "    num_steps_trained: 1455\n",
            "  iterations_since_restore: 291\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 75.7\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0859069423259953\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.038779074517098\n",
            "    mean_inference_ms: 1.2830188825290605\n",
            "    mean_raw_obs_processing_ms: 6.780443953681891\n",
            "  time_since_restore: 70.97966575622559\n",
            "  time_this_iter_s: 0.24150609970092773\n",
            "  time_total_s: 70.97966575622559\n",
            "  timers:\n",
            "    learn_throughput: 42.509\n",
            "    learn_time_ms: 117.622\n",
            "    sample_throughput: 218.703\n",
            "    sample_time_ms: 22.862\n",
            "    update_time_ms: 3.055\n",
            "  timestamp: 1622822706\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1455\n",
            "  training_iteration: 291\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         70.9797</td><td style=\"text-align: right;\">1455</td><td style=\"text-align: right;\"> 5.86641</td><td style=\"text-align: right;\">                 150</td><td style=\"text-align: right;\">           -0.141046</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1550\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-11\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 150.0\n",
            "  episode_reward_mean: 5.751158095733486\n",
            "  episode_reward_min: -0.1410459587955626\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 310\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0036691445857286453\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.470235675573349\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001588011858984828\n",
            "          model: {}\n",
            "          policy_loss: -0.004791200160980225\n",
            "          total_loss: 0.2742980122566223\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 0.27908334136009216\n",
            "    num_agent_steps_sampled: 1550\n",
            "    num_steps_sampled: 1550\n",
            "    num_steps_trained: 1550\n",
            "  iterations_since_restore: 310\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08593435563177584\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.0387584796347453\n",
            "    mean_inference_ms: 1.2797299438979521\n",
            "    mean_raw_obs_processing_ms: 6.331715551316292\n",
            "  time_since_restore: 75.60465407371521\n",
            "  time_this_iter_s: 0.22951054573059082\n",
            "  time_total_s: 75.60465407371521\n",
            "  timers:\n",
            "    learn_throughput: 41.764\n",
            "    learn_time_ms: 119.72\n",
            "    sample_throughput: 235.885\n",
            "    sample_time_ms: 21.197\n",
            "    update_time_ms: 2.818\n",
            "  timestamp: 1622822711\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1550\n",
            "  training_iteration: 310\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         75.6047</td><td style=\"text-align: right;\">1550</td><td style=\"text-align: right;\"> 5.75116</td><td style=\"text-align: right;\">                 150</td><td style=\"text-align: right;\">           -0.141046</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1645\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-17\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 137.0\n",
            "  episode_reward_mean: 5.583295153334322\n",
            "  episode_reward_min: -0.1410459587955626\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 329\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.040715298790019e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.21518540382385254\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00010625070717651397\n",
            "          model: {}\n",
            "          policy_loss: -0.0017164051532745361\n",
            "          total_loss: 0.142826110124588\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 0.14454251527786255\n",
            "    num_agent_steps_sampled: 1645\n",
            "    num_steps_sampled: 1645\n",
            "    num_steps_trained: 1645\n",
            "  iterations_since_restore: 329\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08603804841590232\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.04102998519313\n",
            "    mean_inference_ms: 1.2780779294808462\n",
            "    mean_raw_obs_processing_ms: 5.946547166646018\n",
            "  time_since_restore: 80.23611235618591\n",
            "  time_this_iter_s: 0.2336893081665039\n",
            "  time_total_s: 80.23611235618591\n",
            "  timers:\n",
            "    learn_throughput: 44.148\n",
            "    learn_time_ms: 113.256\n",
            "    sample_throughput: 224.159\n",
            "    sample_time_ms: 22.306\n",
            "    update_time_ms: 3.641\n",
            "  timestamp: 1622822717\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1645\n",
            "  training_iteration: 329\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         80.2361</td><td style=\"text-align: right;\">1645</td><td style=\"text-align: right;\">  5.5833</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">           -0.141046</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1740\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-22\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 137.0\n",
            "  episode_reward_mean: 5.57863223511298\n",
            "  episode_reward_min: -0.1410459587955626\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 348\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0004964537802152336\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.8982102870941162\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.06140489503741264\n",
            "          model: {}\n",
            "          policy_loss: -0.21536441147327423\n",
            "          total_loss: 0.24268946051597595\n",
            "          vf_explained_var: -0.25807809829711914\n",
            "          vf_loss: 0.45802339911460876\n",
            "    num_agent_steps_sampled: 1740\n",
            "    num_steps_sampled: 1740\n",
            "    num_steps_trained: 1740\n",
            "  iterations_since_restore: 348\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.4\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08613239902048589\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.048494652860467\n",
            "    mean_inference_ms: 1.2771257012042625\n",
            "    mean_raw_obs_processing_ms: 5.611993486355545\n",
            "  time_since_restore: 84.87105464935303\n",
            "  time_this_iter_s: 0.2496950626373291\n",
            "  time_total_s: 84.87105464935303\n",
            "  timers:\n",
            "    learn_throughput: 43.198\n",
            "    learn_time_ms: 115.747\n",
            "    sample_throughput: 217.093\n",
            "    sample_time_ms: 23.032\n",
            "    update_time_ms: 2.645\n",
            "  timestamp: 1622822722\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1740\n",
            "  training_iteration: 348\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         84.8711</td><td style=\"text-align: right;\">1740</td><td style=\"text-align: right;\"> 5.57863</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">           -0.141046</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1835\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-27\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 137.0\n",
            "  episode_reward_mean: 5.579650977943752\n",
            "  episode_reward_min: -0.1410459587955626\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 367\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.04076138883829117\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4651680886745453\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.05377843230962753\n",
            "          model: {}\n",
            "          policy_loss: -0.11333165317773819\n",
            "          total_loss: -0.0398925319314003\n",
            "          vf_explained_var: -0.546963095664978\n",
            "          vf_loss: 0.07124700397253036\n",
            "    num_agent_steps_sampled: 1835\n",
            "    num_steps_sampled: 1835\n",
            "    num_steps_trained: 1835\n",
            "  iterations_since_restore: 367\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08623007065701743\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.0581459378152807\n",
            "    mean_inference_ms: 1.2769828315594571\n",
            "    mean_raw_obs_processing_ms: 5.318954766268447\n",
            "  time_since_restore: 89.57766199111938\n",
            "  time_this_iter_s: 0.2318120002746582\n",
            "  time_total_s: 89.57766199111938\n",
            "  timers:\n",
            "    learn_throughput: 41.559\n",
            "    learn_time_ms: 120.311\n",
            "    sample_throughput: 217.135\n",
            "    sample_time_ms: 23.027\n",
            "    update_time_ms: 3.881\n",
            "  timestamp: 1622822727\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1835\n",
            "  training_iteration: 367\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         89.5777</td><td style=\"text-align: right;\">1835</td><td style=\"text-align: right;\"> 5.57965</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">           -0.141046</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 1930\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-32\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 137.0\n",
            "  episode_reward_mean: 5.896319899779285\n",
            "  episode_reward_min: -0.2\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 386\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.9832403659820557\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.1040265560150146\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.02567962370812893\n",
            "          model: {}\n",
            "          policy_loss: -0.17617864906787872\n",
            "          total_loss: 2713.349609375\n",
            "          vf_explained_var: 0.2836701273918152\n",
            "          vf_loss: 2713.47509765625\n",
            "    num_agent_steps_sampled: 1930\n",
            "    num_steps_sampled: 1930\n",
            "    num_steps_trained: 1930\n",
            "  iterations_since_restore: 386\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08687241064306489\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.069762198642434\n",
            "    mean_inference_ms: 1.276571557602014\n",
            "    mean_raw_obs_processing_ms: 5.059787272304863\n",
            "  time_since_restore: 94.30501651763916\n",
            "  time_this_iter_s: 0.24225401878356934\n",
            "  time_total_s: 94.30501651763916\n",
            "  timers:\n",
            "    learn_throughput: 43.982\n",
            "    learn_time_ms: 113.683\n",
            "    sample_throughput: 191.589\n",
            "    sample_time_ms: 26.098\n",
            "    update_time_ms: 2.883\n",
            "  timestamp: 1622822732\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 1930\n",
            "  training_iteration: 386\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">          94.305</td><td style=\"text-align: right;\">1930</td><td style=\"text-align: right;\"> 5.89632</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">                -0.2</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2025\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-37\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 137.0\n",
            "  episode_reward_mean: 5.956194729287258\n",
            "  episode_reward_min: -0.2\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 405\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.8825289011001587\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.8675780296325684\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.018008893355727196\n",
            "          model: {}\n",
            "          policy_loss: -0.11954104900360107\n",
            "          total_loss: 0.0023887394927442074\n",
            "          vf_explained_var: 0.168523371219635\n",
            "          vf_loss: 0.08802751451730728\n",
            "    num_agent_steps_sampled: 2025\n",
            "    num_steps_sampled: 2025\n",
            "    num_steps_trained: 2025\n",
            "  iterations_since_restore: 405\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.4\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0875005377947418\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.080191187995389\n",
            "    mean_inference_ms: 1.2773564198839218\n",
            "    mean_raw_obs_processing_ms: 4.828753497863754\n",
            "  time_since_restore: 99.09721088409424\n",
            "  time_this_iter_s: 0.23060917854309082\n",
            "  time_total_s: 99.09721088409424\n",
            "  timers:\n",
            "    learn_throughput: 45.019\n",
            "    learn_time_ms: 111.065\n",
            "    sample_throughput: 219.373\n",
            "    sample_time_ms: 22.792\n",
            "    update_time_ms: 3.082\n",
            "  timestamp: 1622822737\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2025\n",
            "  training_iteration: 405\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         99.0972</td><td style=\"text-align: right;\">2025</td><td style=\"text-align: right;\"> 5.95619</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">                -0.2</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2115\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-42\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 6.056504108012375\n",
            "  episode_reward_min: -0.2\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 423\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.5738635063171387\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.8488719463348389\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.016675403341650963\n",
            "          model: {}\n",
            "          policy_loss: -0.18986190855503082\n",
            "          total_loss: 1.1526296138763428\n",
            "          vf_explained_var: -1.0\n",
            "          vf_loss: 1.2828959226608276\n",
            "    num_agent_steps_sampled: 2115\n",
            "    num_steps_sampled: 2115\n",
            "    num_steps_trained: 2115\n",
            "  iterations_since_restore: 423\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08801338791901639\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.0925834042886517\n",
            "    mean_inference_ms: 1.2779682416018976\n",
            "    mean_raw_obs_processing_ms: 4.632201031397556\n",
            "  time_since_restore: 103.69997978210449\n",
            "  time_this_iter_s: 0.2585480213165283\n",
            "  time_total_s: 103.69997978210449\n",
            "  timers:\n",
            "    learn_throughput: 42.292\n",
            "    learn_time_ms: 118.227\n",
            "    sample_throughput: 219.987\n",
            "    sample_time_ms: 22.729\n",
            "    update_time_ms: 3.002\n",
            "  timestamp: 1622822742\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2115\n",
            "  training_iteration: 423\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">           103.7</td><td style=\"text-align: right;\">2115</td><td style=\"text-align: right;\">  6.0565</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.2</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2205\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-47\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 5.856204418141635\n",
            "  episode_reward_min: -0.39999999999999997\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 441\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.6803975105285645\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.3643990755081177\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.007421419024467468\n",
            "          model: {}\n",
            "          policy_loss: -0.15393026173114777\n",
            "          total_loss: 0.5875455737113953\n",
            "          vf_explained_var: -0.7447265386581421\n",
            "          vf_loss: 0.7215834856033325\n",
            "    num_agent_steps_sampled: 2205\n",
            "    num_steps_sampled: 2205\n",
            "    num_steps_trained: 2205\n",
            "  iterations_since_restore: 441\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08855848084769108\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.1054946292896433\n",
            "    mean_inference_ms: 1.2793075261299394\n",
            "    mean_raw_obs_processing_ms: 4.45390011980771\n",
            "  time_since_restore: 108.38828802108765\n",
            "  time_this_iter_s: 0.25226879119873047\n",
            "  time_total_s: 108.38828802108765\n",
            "  timers:\n",
            "    learn_throughput: 43.074\n",
            "    learn_time_ms: 116.078\n",
            "    sample_throughput: 198.125\n",
            "    sample_time_ms: 25.237\n",
            "    update_time_ms: 2.953\n",
            "  timestamp: 1622822747\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2205\n",
            "  training_iteration: 441\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         108.388</td><td style=\"text-align: right;\">2205</td><td style=\"text-align: right;\">  5.8562</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2295\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-53\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 5.872443859852986\n",
            "  episode_reward_min: -0.39999999999999997\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 459\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.5231709480285645\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 2.0307698249816895\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.010831372812390327\n",
            "          model: {}\n",
            "          policy_loss: -0.10823900997638702\n",
            "          total_loss: -0.05384896323084831\n",
            "          vf_explained_var: 0.9740583300590515\n",
            "          vf_loss: 0.0053978837095201015\n",
            "    num_agent_steps_sampled: 2295\n",
            "    num_steps_sampled: 2295\n",
            "    num_steps_trained: 2295\n",
            "  iterations_since_restore: 459\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.9\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08908958909622365\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.1186111947863453\n",
            "    mean_inference_ms: 1.2801210593033556\n",
            "    mean_raw_obs_processing_ms: 4.29127290249895\n",
            "  time_since_restore: 113.08928155899048\n",
            "  time_this_iter_s: 0.25654101371765137\n",
            "  time_total_s: 113.08928155899048\n",
            "  timers:\n",
            "    learn_throughput: 44.743\n",
            "    learn_time_ms: 111.748\n",
            "    sample_throughput: 195.51\n",
            "    sample_time_ms: 25.574\n",
            "    update_time_ms: 2.911\n",
            "  timestamp: 1622822753\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2295\n",
            "  training_iteration: 459\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         113.089</td><td style=\"text-align: right;\">2295</td><td style=\"text-align: right;\"> 5.87244</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2385\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-05-58\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 5.397387518476312\n",
            "  episode_reward_min: -0.39999999999999997\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 477\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.544283628463745\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.9029783010482788\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.008474061265587807\n",
            "          model: {}\n",
            "          policy_loss: -0.13004449009895325\n",
            "          total_loss: -0.05410902574658394\n",
            "          vf_explained_var: 0.5739458203315735\n",
            "          vf_loss: 0.05437503382563591\n",
            "    num_agent_steps_sampled: 2385\n",
            "    num_steps_sampled: 2385\n",
            "    num_steps_trained: 2385\n",
            "  iterations_since_restore: 477\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08941651351268315\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.1304850275386893\n",
            "    mean_inference_ms: 1.2807238294555225\n",
            "    mean_raw_obs_processing_ms: 4.142719426070254\n",
            "  time_since_restore: 117.82649397850037\n",
            "  time_this_iter_s: 0.21543359756469727\n",
            "  time_total_s: 117.82649397850037\n",
            "  timers:\n",
            "    learn_throughput: 42.674\n",
            "    learn_time_ms: 117.168\n",
            "    sample_throughput: 230.573\n",
            "    sample_time_ms: 21.685\n",
            "    update_time_ms: 3.664\n",
            "  timestamp: 1622822758\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2385\n",
            "  training_iteration: 477\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         117.826</td><td style=\"text-align: right;\">2385</td><td style=\"text-align: right;\"> 5.39739</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2475\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-06-03\n",
            "  done: false\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 5.471582787941437\n",
            "  episode_reward_min: -0.39999999999999997\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 495\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.9082127809524536\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.524962067604065\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.04171771556138992\n",
            "          model: {}\n",
            "          policy_loss: -0.1709347665309906\n",
            "          total_loss: -0.04545839875936508\n",
            "          vf_explained_var: 0.9994752407073975\n",
            "          vf_loss: 0.04587011784315109\n",
            "    num_agent_steps_sampled: 2475\n",
            "    num_steps_sampled: 2475\n",
            "    num_steps_trained: 2475\n",
            "  iterations_since_restore: 495\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08958739721293282\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.139946781582998\n",
            "    mean_inference_ms: 1.281357746820026\n",
            "    mean_raw_obs_processing_ms: 4.00624342717058\n",
            "  time_since_restore: 122.49352526664734\n",
            "  time_this_iter_s: 0.2517728805541992\n",
            "  time_total_s: 122.49352526664734\n",
            "  timers:\n",
            "    learn_throughput: 44.391\n",
            "    learn_time_ms: 112.634\n",
            "    sample_throughput: 203.094\n",
            "    sample_time_ms: 24.619\n",
            "    update_time_ms: 3.069\n",
            "  timestamp: 1622822763\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2475\n",
            "  training_iteration: 495\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>RUNNING </td><td>172.28.0.2:9274</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         122.494</td><td style=\"text-align: right;\">2475</td><td style=\"text-align: right;\"> 5.47158</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_compiler_gym_6c005_00000:\n",
            "  agent_timesteps_total: 2500\n",
            "  custom_metrics: {}\n",
            "  date: 2021-06-04_16-06-04\n",
            "  done: true\n",
            "  episode_len_mean: 5.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 144.0\n",
            "  episode_reward_mean: 5.522230054957476\n",
            "  episode_reward_min: -0.39999999999999997\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 500\n",
            "  experiment_id: 7776d75152b847b49ae1e786872125af\n",
            "  hostname: 1c59bc4276bf\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.1467392444610596\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 1.6955039501190186\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.021883441135287285\n",
            "          model: {}\n",
            "          policy_loss: -0.2207040786743164\n",
            "          total_loss: -0.07645255327224731\n",
            "          vf_explained_var: 0.45980769395828247\n",
            "          vf_loss: 0.09727345407009125\n",
            "    num_agent_steps_sampled: 2500\n",
            "    num_steps_sampled: 2500\n",
            "    num_steps_trained: 2500\n",
            "  iterations_since_restore: 500\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf: {}\n",
            "  pid: 9274\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.08965859823459468\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 2.14245491781871\n",
            "    mean_inference_ms: 1.2814355218481577\n",
            "    mean_raw_obs_processing_ms: 3.9703069860459044\n",
            "  time_since_restore: 123.80238914489746\n",
            "  time_this_iter_s: 0.27312183380126953\n",
            "  time_total_s: 123.80238914489746\n",
            "  timers:\n",
            "    learn_throughput: 42.858\n",
            "    learn_time_ms: 116.665\n",
            "    sample_throughput: 203.113\n",
            "    sample_time_ms: 24.617\n",
            "    update_time_ms: 3.096\n",
            "  timestamp: 1622822764\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 2500\n",
            "  training_iteration: 500\n",
            "  trial_id: 6c005_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.38 GiB heap, 0.0/3.69 GiB objects<br>Result logdir: /root/ray_results/PPO_2021-06-04_16-03-37<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_compiler_gym_6c005_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         123.802</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\"> 5.52223</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m 2021-06-04 16:06:04,955\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     ray.actor.exit_actor()\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/actor.py\", line 1077, in exit_actor\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     raise exit\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m SystemExit: 0\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m During handling of the above exception, another exception occurred:\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/traceback.py\", line 167, in format_exc\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/traceback.py\", line 121, in format_exception\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     capture_locals=capture_locals)\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     f.line\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     lines = getlines(filename, module_globals)\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     return updatecache(filename, module_globals)\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/linecache.py\", line 137, in updatecache\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     lines = fp.readlines()\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/lib/python3.7/codecs.py\", line 319, in decode\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     def decode(self, input, final=False):\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 379, in sigterm_handler\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m     sys.exit(1)\n",
            "\u001b[2m\u001b[36m(pid=9274)\u001b[0m SystemExit: 1\n",
            "2021-06-04 16:06:05,056\tINFO tune.py:549 -- Total run time: 147.61 seconds (147.34 seconds for the tuning loop).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFDxXezNuXdy"
      },
      "source": [
        "## Evaluate the agent\n",
        "\n",
        "After running the training loop we can create a new agent that has exploration disabled, restore it from the training checkpoint, and then use it for running inference tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mYoBhSEi26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dae3ec2-9d0e-4919-86b9-10362d9dd5ee"
      },
      "source": [
        "agent = PPOTrainer(\n",
        "    env=\"compiler_gym\",\n",
        "    config={\n",
        "        \"num_workers\": 1,\n",
        "        \"seed\": 0xCC,\n",
        "        # For inference we disable the stocastic exploration that is used during \n",
        "        # training.\n",
        "        \"explore\": False,\n",
        "    },\n",
        ")\n",
        "\n",
        "# We only made a single checkpoint at the end of training, so restore that. In\n",
        "# practice we may have many checkpoints that we will select from using \n",
        "# performance on the validation set.\n",
        "checkpoint = analysis.get_best_checkpoint(\n",
        "    metric=\"episode_reward_mean\", \n",
        "    mode=\"max\", \n",
        "    trial=analysis.trials[0]\n",
        ")\n",
        "\n",
        "agent.restore(checkpoint)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-04 16:06:05,179\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2021-06-04 16:06:05,181\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/variadic_reduce.py:115: calling function (from tensorflow.python.eager.def_function) with experimental_compile is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=10320)\u001b[0m experimental_compile is deprecated, use jit_compile instead\n",
            "2021-06-04 16:06:12,448\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
            "2021-06-04 16:06:12,520\tINFO trainable.py:378 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/PPO_2021-06-04_16-03-37/PPO_compiler_gym_6c005_00000_0_2021-06-04_16-03-37/checkpoint_000500/checkpoint-500\n",
            "2021-06-04 16:06:12,521\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 500, '_timesteps_total': None, '_time_total': 123.80238914489746, '_episodes_total': 500}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWeLEVYZjVuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee293b46-650f-4fbb-8ff4-4414eb4140c5"
      },
      "source": [
        "# Lets define a helper function to make it easy to evaluate the agent's \n",
        "# performance on a set of benchmarks.\n",
        "\n",
        "def run_agent_on_benchmarks(benchmarks):\n",
        "  \"\"\"Run agent on a list of benchmarks and return a list of cumulative rewards.\"\"\"\n",
        "  with make_env() as env:\n",
        "    rewards = []\n",
        "    for i, benchmark in enumerate(benchmarks, start=1):\n",
        "        observation, done = env.reset(benchmark=benchmark), False\n",
        "        while not done:\n",
        "            action = agent.compute_action(observation)\n",
        "            observation, _, done, _ = env.step(action)\n",
        "        rewards.append(env.episode_reward)\n",
        "        print(f\"[{i}/{len(benchmarks)}] {env.state}\")\n",
        "\n",
        "  return rewards\n",
        "\n",
        "# Evaluate agent performance on the validation set.\n",
        "val_rewards = run_agent_on_benchmarks(val_benchmarks)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/5] benchmark='benchmark://npb-v0/51' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.04497957229614258 reward=0.8457711442786069\n",
            "[2/5] benchmark='benchmark://npb-v0/52' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.02101874351501465 reward=0.5414012738853503\n",
            "[3/5] benchmark='benchmark://npb-v0/53' commandline='opt -newgvn -jump-threading -jump-threading -jump-threading -jump-threading input.bc -o output.bc' walltime=0.03879737854003906 reward=1.1529896907216495\n",
            "[4/5] benchmark='benchmark://npb-v0/54' commandline='opt -newgvn -instcombine -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.011642217636108398 reward=0.8873239436619719\n",
            "[5/5] benchmark='benchmark://npb-v0/55' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.016828536987304688 reward=0.487012987012987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEc872g10UmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96170474-0742-4e7c-9d53-4d2fc094f9fb"
      },
      "source": [
        "# Evaluate agent performance on the holdout test set.\n",
        "test_rewards = run_agent_on_benchmarks(test_benchmarks)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/12] benchmark='benchmark://chstone-v0/adpcm' commandline='opt -newgvn -newgvn -newgvn -newgvn -newgvn input.bc -o output.bc' walltime=0.03381752967834473 reward=0.5059055118110236\n",
            "[2/12] benchmark='benchmark://chstone-v0/aes' commandline='opt -newgvn -newgvn -newgvn -newgvn -newgvn input.bc -o output.bc' walltime=0.04671216011047363 reward=0.7559437166424066\n",
            "[3/12] benchmark='benchmark://chstone-v0/blowfish' commandline='opt -newgvn -jump-threading -jump-threading -jump-threading -jump-threading input.bc -o output.bc' walltime=0.018262624740600586 reward=0.6108695652173912\n",
            "[4/12] benchmark='benchmark://chstone-v0/dfadd' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.01693105697631836 reward=0.4024604569420035\n",
            "[5/12] benchmark='benchmark://chstone-v0/dfdiv' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.018264055252075195 reward=0.3904320987654321\n",
            "[6/12] benchmark='benchmark://chstone-v0/dfmul' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.016455411911010742 reward=0.39019607843137255\n",
            "[7/12] benchmark='benchmark://chstone-v0/dfsin' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.023482561111450195 reward=0.40222984562607206\n",
            "[8/12] benchmark='benchmark://chstone-v0/gsm' commandline='opt -newgvn -jump-threading -jump-threading -jump-threading -jump-threading input.bc -o output.bc' walltime=0.018106698989868164 reward=0.515527950310559\n",
            "[9/12] benchmark='benchmark://chstone-v0/jpeg' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.036251068115234375 reward=0.47243107769423553\n",
            "[10/12] benchmark='benchmark://chstone-v0/mips' commandline='opt -newgvn -jump-threading -jump-threading -jump-threading -jump-threading input.bc -o output.bc' walltime=0.01343846321105957 reward=0.5450819672131147\n",
            "[11/12] benchmark='benchmark://chstone-v0/motion' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.0157320499420166 reward=0.4491682070240296\n",
            "[12/12] benchmark='benchmark://chstone-v0/sha' commandline='opt -newgvn -early-cse-memssa -early-cse-memssa -early-cse-memssa -early-cse-memssa input.bc -o output.bc' walltime=0.01633453369140625 reward=0.42857142857142855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo3Dn360EE5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "55879094-dcc6-4d43-f385-d71db1300efe"
      },
      "source": [
        "# Finally lets plot our results to see how we did!\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_results(x, y, name, ax):\n",
        "  plt.sca(ax)\n",
        "  plt.bar(range(len(y)), y)\n",
        "  plt.ylabel(\"Reward (higher is better)\")\n",
        "  plt.xticks(range(len(x)), x, rotation = 90)\n",
        "  plt.title(f\"Performance on {name} set\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.set_size_inches(13, 3)\n",
        "plot_results(val_benchmarks, val_rewards, \"val\", ax1)\n",
        "plot_results(test_benchmarks, test_rewards, \"test\", ax2)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1lXpJbA04Ji"
      },
      "source": [
        "That's it for this demonstration! Check out the [documentation site](https://facebookresearch.github.io/CompilerGym/) for more details, API reference, and more. If you can encounter any problems, please [file an issue](https://github.com/facebookresearch/CompilerGym/issues)."
      ]
    }
  ]
}
